hw2
Cifar-10資料集，以CNN卷積式神經網路進行訓練

文件結構(recommended by DeepSeek):
```
cifar10_cnn/
├── data/                # 存放數據集
├── models/              # 保存訓練好的模型
├── utils/               # 工具函數
│   └── preprocess.py    # 數據預處理函數
├── experiments/         # 不同實驗
│   ├── test2_normalization.py
│   ├── test3_shuffle.py
│   └── ...
├── config.py            # 共用配置
└── main.py              # 主入口文件
```

## 測試一：固定模型架構及參數下，資料是否正規化之比較。
![alt text](experiments/normalization_comparison.png)

| 指標                | 未正規化數據 | 正規化數據 | 改善幅度 |
|---------------------|--------------|------------|----------|
| 最終驗證準確率       | 0.62         | 0.68       | +9.7%    |
| 最終驗證損失         | 1.25         | 0.95       | -24%     |
| 收斔所需 epochs      | 6            | 3          | 快 2 倍  |
在CNN訓練中，輸入數據正規化是必要步驟，尤其當像素值範圍較大時(如 0-255)。本測試顯示正規化可同時提升準確率、加速收斂，並降低訓練不穩定性。

## 測試二：固定模型架構及參數下，資料是否進行shuffle之比較。
#### **控制變因**  
- 模型架構(與測試一相同)
- 批次大小(`batch_size=64`)
- Epoch 數(`epochs=10`)
- 優化器與學習率(Adam,預設)  

#### **操縱變因**  
- **有 Shuffle**：`model.fit(shuffle=True)`  
- **無 Shuffle**：`model.fit(shuffle=False)`  
![alt text](experiments/shuffle_comparison.png)

| 指標                | Shuffle=True | Shuffle=False | 差異分析               |
|---------------------|--------------|---------------|------------------------|
| 最終驗證準確率       | 0.68         | 0.62          | +6%                   |
| 最終驗證損失         | 0.95         | 1.15          | -17%                  |
| 訓練穩定性           | 高           | 低（有震盪）   | Shuffle 減少梯度波動   |
測試結果表明，在訓練CNN模型時啟用資料Shuffle能顯著提升模型效能，最終驗證準確率提高約6%，同時使訓練過程更穩定、收斂更快。

## 測試三：固定參數下，卷積層層數多寡之比較。至少三種網路架構，例如：兩層、三層、四層。
- **控制變因**：  
  - 使用相同的優化器（Adam）、學習率、批次大小（batch_size=64）、正規化（像素值縮放到 0-1）  
  - 固定其他層結構（MaxPooling2D、Flatten、Dense）  
- **操縱變因**：  
  - **2層 CNN**：1 Conv+Pool → 1 Conv+Pool → Flatten  
  - **3層 CNN**：1 Conv+Pool → 1 Conv+Pool → 1 Conv（不加 Pool）→ Flatten  
  - **4層 CNN**：1 Conv+Pool → 1 Conv+Pool → 1 Conv（不加 Pool）→ 1 Conv（不加 Pool）→ Flatten  

![alt text](experiments/cnn_layers_comparison.png)
| 模型架構   | 最終驗證準確率 | 最終驗證損失 | 訓練準確率 | 關鍵觀察                     |
|------------|----------------|--------------|------------|------------------------------|
| **2層CNN** | 70.71%         | 0.8879       | 80.30%     | 訓練集過擬合明顯(+9.59%)   |
| **3層CNN** | 73.17%         | 0.9160       | 87.07%     | 最佳泛化能力，但後期波動     |
| **4層CNN** | 73.78%         | 0.8593       | 87.38%     | 最高準確率，但過擬合風險最大 |
- **收斂速度**  
  - 所有模型在Epoch 3-4後驗證準確率趨穩，說明**CIFAR-10在10個Epoch內可達初步收斂**。
  - 4層CNN初期損失下降最快，但後期過擬合。

- **過擬合跡象**  
  - 2層/4層模型的訓練準確率比驗證準確率高出 **10%以上**，3層差距最小(13.9%)。
### 途中問題
  - 輸入圖像尺寸：32x32  
  - 經過 `Conv2D + MaxPooling2D` 兩次後：  
    - 第一次：32x32 → 30x30 (kernel=3x3, padding='valid') → 15x15 (MaxPooling 2x2)  
    - 第二次：15x15 → 13x13 → 6x6  
    - 第三次：6x6 → 4x4 → 2x2  
    - 第四次：2x2 → **無法再進行 3x3 卷積**（因為 2 - 3 + 1 = 0）。  

- **修正後**：  
  - 使用 `padding="same"` 讓卷積層不縮小尺寸（例如 32x32 → 32x32）。  
  - 減少池化層次數，避免特徵圖變得太小。

## 測試四：固定層數、其他參數下，調整filters大小之比較。至少三種數值，例如：16、32、64。
### **1. 實驗設計**
- **控制變因**：  
  - 固定 3 層 CNN 架構（Conv → Pool → Conv → Pool → Conv → Flatten → Dense）  
  - 固定超參數：`epochs=10`, `batch_size=64`, 優化器 (`Adam`), 學習率 (預設值)  
  - 輸入數據正規化 (`x_train/255.0`)  
- **操縱變因**：  
  - 卷積核數量：`filters=[16, 32, 64]`

![alt text](experiments/filters_comparison.png)

| 模型配置       | 最終訓練準確率 | 最終驗證準確率 | 驗證準確率峰值 | 過擬合程度（訓練-驗證差距） |
|----------------|----------------|----------------|----------------|-----------------------------|
| **filters=16** | 83.69%         | 71.85%         | 72.76% (Epoch7)| 11.84%                      |
| **filters=32** | 88.98%         | 72.82%         | 74.24% (Epoch8)| 16.16%                      |
| **filters=64** | 94.80%         | 74.60%         | 74.99% (Epoch9)| 20.20%                      |

- **準確率與容量關係**
   - filters 數量與模型性能呈正相關：
     - 16→32：驗證準確率提升 0.97%
     - 32→64：驗證準確率提升 1.78%
   - filters=64 達到最高驗證準確率 74.6%，但同時伴隨嚴重過擬合（訓練準確率 94.8%）

- **訓練效率對比**
   - 收斂速度：
     - filters=16 最快穩定（Epoch5 達 70%）
     - filters=64 前期收斂快，但後期不穩定
   - 計算成本：
     - filters=64 的每 epoch 時間比 16 多 40%
